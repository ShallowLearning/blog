<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Shallow Learning</title><link href="https://shallowlearning.github.io/blog/" rel="alternate"></link><link href="https://shallowlearning.github.io/blog/feeds/tag/statistics.atom.xml" rel="self"></link><id>https://shallowlearning.github.io/blog/</id><updated>2015-07-15T00:00:00-04:00</updated><entry><title>This is Not the Terminator You are LookingÂ For</title><link href="https://shallowlearning.github.io/blog/posts/2015/07/this-is-not-the-terminator-you-are-looking-for/" rel="alternate"></link><updated>2015-07-15T00:00:00-04:00</updated><author><name>Kevin Wang</name></author><id>tag:shallowlearning.github.io,2015-07-15:blog/posts/2015/07/this-is-not-the-terminator-you-are-looking-for/</id><summary type="html">&lt;p&gt;Welcome to Shallow Learning.  I plan to write a number of posts on the theme of machine learning and statistics to help people gain a more intuitive understanding of various phenomena around us.  I&amp;#8217;m going to start with a topic that has recently gained a lot of traction in the media; the perception of the pressing danger of machine artificial intelligence (&lt;span class="caps"&gt;AI&lt;/span&gt;) concerning humanity. Going forward Josh and I will touch on many of the abstractions present and implied by the&amp;nbsp;debate. &lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img src="/images/sky_net.jpg" title="'Judgement Day is coming"" height="200" width="400" alt="'Judgement Day is coming"" class="mid"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Typically when people not directly working on &lt;span class="caps"&gt;AI&lt;/span&gt; or machine learning think about a the dangerous &lt;span class="caps"&gt;AI&lt;/span&gt; they think about a conscious malevolent superintelligence like Skynet (actually shockingly dumb, which is another problem altogether).  A long list of notables including Elon Musk, Ray Kurzweil, Bill Gates, and Stephen Hawking have expressed great concern about this topic&lt;sup&gt;&lt;a href="https://www.washingtonpost.com/blogs/the-switch/wp/2015/03/24/apple-co-founder-on-artificial-intelligence-the-future-is-scary-and-very-bad-for-people/"&gt;1&lt;/a&gt;&lt;a href="http://simplystatistics.org/2015/05/18/residual-expertise/"&gt;2&lt;/a&gt;&lt;/sup&gt;.  Polls are being taken of when Judgement Day will happen with scary results&lt;sup&gt;&lt;a href="http://www.nickbostrom.com/papers/survey.pdf"&gt;3&lt;/a&gt;&lt;/sup&gt;. So why is this blog called Shallow Learning and not simply written by a computer? Simply put, while there have been amazing advances in &lt;span class="caps"&gt;AI&lt;/span&gt; research lately, the open problems in mimicking 1 billion years of evolution are incredibly hard if not intractable to solve. To begin to explain the current constraints of &lt;span class="caps"&gt;AI&lt;/span&gt; and why I am not worried about this result while working in the field, I need to introduce a few terms and limit my focus to the advances in Machine learning or statistical approaches to &lt;span class="caps"&gt;A.I.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Statistical approaches to &lt;span class="caps"&gt;A.I.&lt;/span&gt; can be divided up into three different categories; supervised learning, unsupervised learning, and reinforcement learning.  Below are my definitions of those&amp;nbsp;categories.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img src="/images/facebook_face_tagger.jpg" title="Pretty sure this is your face" height="200" width="400" alt="Pretty sure this is your face" class="mid"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Supervised learning - Given data that has been labeled in some manner the algorithm analyzes and infers the a function that can then infer the labels for unlabeled data. Psychologists also call this concept&amp;nbsp;learning.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img src="/images/cat_deep_learning.jpg" title="16 percent sure this is something" height="200" width="400" alt="16 percent sure this is something" class="mid"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Unsupervised learning - Given unlabeled data the algorithm analyzes and draws inferences from said&amp;nbsp;data&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img src="/images/deepblue.jpg" title="This action is the best one" height="200" width="400" alt="This action is the best one" class="mid"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Reinforcement learning - Given data an agent goes through trial and error drawing inferences through interactions or rewards and&amp;nbsp;punishments&lt;/p&gt;
&lt;p&gt;Shallow Learning will have posts that cover these far more in depth, but for now we&amp;#8217;re going to focus on the hype and limitation of deep learning: a supervised learning technique that has matched or exceeded human performance in many subtasks that measure human&amp;nbsp;intelligence.&lt;/p&gt;
&lt;p&gt;The definition of deep learning I prefer is an algorithm that can automatically learn concepts or feature representations through experience or a large number of examples. Media uses it as the ultimate buzz/scare-word as well as industry.  It is the only viable approach to building &lt;span class="caps"&gt;AI&lt;/span&gt; systems that can operate in complicated real world environments&lt;sup&gt;&lt;a href="http://www.iro.umontreal.ca/~bengioy/dlbook/front_matter.pdf"&gt;4&lt;/a&gt;&lt;/sup&gt;.  Without getting too much into the details (a later post will cover this), the base algorithm for deep learning is currently the neural network, which can be described as layers of logistic regression functions.  Though extremely successful, this approach has many problems including perspective invariance, memory, limited inference in comparison to humans, and flexible hierarchical learning just to name a&amp;nbsp;few.&lt;/p&gt;
&lt;p&gt;These problems are nontrivial and necessary in representing human intelligence correctly.  A machine that cannot recognize a your friend except through her face is not a very good approximation of intelligence.  Finding efficient and viable ways for neural networks to retain stateful information as you do everytime you build a mental narrative or process the semantic relationships between words (like now) is another process necessary for mimic&amp;#8217;ing human ability that is being researched.  Yet another higher order inference that is hard for machines to even frame is inferring social situations from cartoons that might not even fully look like humans (Josh Tenenbaum likes to call this rich learning&lt;sup&gt;&lt;a href="http://web.mit.edu/cocosci/Papers/tkgg-science11-reprint.pdf"&gt;6&lt;/a&gt;&lt;/sup&gt;).  The choice as to whether to model information in a hierarchial structure that is unconstrained by human choice.  Lastly, other large concerns are limitations in reasoning, representation of knowledge and sentience in the current methodologies of machine learning make the roadmap toward &lt;span class="caps"&gt;AGI&lt;/span&gt; a nontrivial endeavor even when ignoring other practical concerns of processing time and&amp;nbsp;integration.&lt;/p&gt;
&lt;p&gt;The statistical algorithms we use for deep learning have a limitation in inference where they are susceptible to being very confident about data that is outside of what it can predict&lt;sup&gt;&lt;a href="http://arxiv.org/pdf/1412.6572.pdf"&gt;7&lt;/a&gt;&lt;/sup&gt;.  This means even if researchers get all other parts of the process correct, you still have a system that cannot identify the gaps in its own reasoning, which is an undesirable form of intelligence. Lastly, limitations on representation of knowledge and sentience are tied together.  Creating a notion of self and identity for a machine that is nontrivial is something that research and philosophy currently has no means of answering.  Marvin Minsky may be correct in that an intelligence is comprised up of many non-intelligent parts with layers of processes that regulate one another, yet currently &lt;span class="caps"&gt;AI&lt;/span&gt; is not comprised of enough working unintelligent parts to even verify the hypothesis.  Integrating the algorithms that eventually prove best for each specific task will be a huge endeavor.  There is much work to be done where intelligence is concerned given that there are more deficiencies in machine learning that we have not encountered&amp;nbsp;yet.&lt;/p&gt;
&lt;p&gt;There are things I&amp;#8217;ve missed and not given proper attention to.  These words are currently all I have on the topic, but the topic is large enough that I&amp;#8217;m sure I&amp;#8217;ve omitted many things.  Let me know in the comments what you&amp;nbsp;think.   &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.washingtonpost.com/blogs/the-switch/wp/2015/03/24/apple-co-founder-on-artificial-intelligence-the-future-is-scary-and-very-bad-for-people/" title="Washington Post woohoo"&gt;One of many articles about famous intlligent people think &lt;span class="caps"&gt;A.I.&lt;/span&gt; is&amp;nbsp;dangerous&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://simplystatistics.org/2015/05/18/residual-expertise/"&gt;There&amp;#8217;s a term for when people believe experts in a field that is not their own and it&amp;#8217;s residual&amp;nbsp;expertise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.nickbostrom.com/papers/survey.pdf"&gt;Survey of when &lt;span class="caps"&gt;AGI&lt;/span&gt; when happen, when &lt;span class="caps"&gt;ASI&lt;/span&gt; will happen, when people will solve &lt;span class="caps"&gt;AIDS&lt;/span&gt;&amp;#8230; (ok the last isn&amp;#8217;t&amp;nbsp;true)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.iro.umontreal.ca/~bengioy/dlbook/front_matter.pdf"&gt;Yoshua Bengio&amp;#8217;s (one of the three fathers of the deep learning movement) comment in his upcoming&amp;nbsp;textbook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cs.toronto.edu/~fritz/absps/transauto6.pdf"&gt;Geoffrey Hinton&amp;#8217;s paper regarding his solution for perspective in&amp;nbsp;vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://web.mit.edu/cocosci/Papers/tkgg-science11-reprint.pdf"&gt;Tenenbaum&amp;#8217;s classic lecture on &amp;#8216;rich inference&amp;#8217; in pdf&amp;nbsp;form&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/pdf/1412.6572.pdf"&gt;Ian Goodfellow&amp;#8217;s Adverserial Examples paper, which formalizes this&amp;nbsp;weakness&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</summary><category term="Machine Learning"></category><category term="Statistics"></category><category term="Deep Learning"></category></entry></feed>